\section{Design}
\subsection{Overview}
A Heresy network consists of multiple individual Dissent networks composed in a tree-structure. Figure \ref{fig:heresy} shows the structure of a Heresy network. The leaves of the tree are users wishing to send messages anonymously. The users are partitioned into groups. Each group consists of a specific set of users, along with a set of Dissent servers. Each group is a Dissent network. In each round, users submit their cipher texts and a usual Dissent round is run in each group. At the end of a single round, one message is output from each group. We treat these as the cipher texts for the next level in the heresy tree. This continues until one message reaches the root Dissent group. At this point, the message is then multicast back down the tree and all users receive it. We refer to this as one complete Heresy round.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{heresy}
\caption{Heresy:hierarchical Dissent groups}
\label{fig:heresy}
\end{figure}

Users onion-encrypt their messages such that at each level, one layer of encryption can be removed. This is done to achieve the desired anonymity goals of the system. As is the case with Dissent, the final output need not be a plain text message, but rather encrypted for a specific recipient.

\subsection{Design Parameters}

An important design parameter is the branching factor of the Heresy tree. Let each Dissent group consist of $k$ clients and $m$ servers. The branching factor, $k$, must be chosen to balance the trade-off between efficiency and anonymity. Since the Dissent implementation works well with small groups clients, we pick $k=100$. Let $l$ denote the number of levels in the heresy tree. With $k=100$ and $l=4$ we can support 100 million clients.

At each level after the first, each group must run $k$ Dissent rounds for each round performed in the level below it. This implies the entire system is bottle-necked by the root Dissent group. This must be done to avoid imposing a global schedule. In the Dissent protocol, each client is assigned a specific slot during which it may send a message. If we did not run multiple instances, the system would need to drop messages. 


